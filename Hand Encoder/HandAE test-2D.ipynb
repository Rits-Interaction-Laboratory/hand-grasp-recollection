{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    #グラフ出力用module\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#BATCH_SIZE = 100\n",
    "#WEIGHT_DECAY = 0.00005\n",
    "#LEARNING_RATE = 0.0001\n",
    "#EPOCH = 100\n",
    "PATH = os.getcwd()\n",
    "#TRAINPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\train\"\n",
    "#TESTPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\test\"\n",
    "#VALIDPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\valid\"\n",
    "\n",
    "#手指姿勢CSVファイル一覧取得\n",
    "dataset_dir = PATH + \"\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, adrs, transform = None):\n",
    "        self.transform = transform\n",
    "        #self.transform2 = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "        self.data = data\n",
    "        self.adrs = adrs\n",
    "        \n",
    "        self.datanum = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #batch_set = []\n",
    "        \n",
    "        i_data = self.data[idx]\n",
    "        i_adrs = self.adrs[idx]\n",
    "        \n",
    "        #print(out_label)\n",
    "        #print(type(i_label))\n",
    "        out_data = np.array(i_data.astype(np.float32))\n",
    "        #out_label.append(i_label.astype(np.float32))\n",
    "        #print(type(out_label))\n",
    "\n",
    "        #batch_set.append(out_data)\n",
    "        #batch_set.append(out_label)\n",
    "        #batch_set.append(out_img)\n",
    "        #batch_set.append(out_mask)\n",
    "        #batch_set.append(out_lmask)\n",
    "        #print(out_data.shape)\n",
    "        #print(out_label.shape)\n",
    "        #print(out_mask.shape)\n",
    "        #print(out_lmask.shape)\n",
    "        \n",
    "        return out_data, i_adrs\n",
    "        #return batch_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ignore(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation = lambda x: x):\n",
    "        '''\n",
    "        引数:\n",
    "            input_dim: 入力次元\n",
    "            output_dim: 出力次元\n",
    "            activation: 活性化関数\n",
    "        パラメータ:\n",
    "            W: 重み\n",
    "            b: バイアス\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.Tensor(np.random.normal(size = (output_dim, input_dim))))\n",
    "        self.b = nn.Parameter(torch.Tensor(np.zeros(output_dim)))\n",
    "        self.activation = activation\n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        out = torch.empty_like(x).to(torch.device(\"cuda:0\"))\n",
    "        \n",
    "        masked_x = torch.mul(x, mask)\n",
    "        \n",
    "        try:\n",
    "            m_size = torch.Tensor(mask.size()[1]).to(torch.device(\"cuda:0\"))\n",
    "            m_sum = torch.sum(mask, 1).to(torch.device(\"cuda:0\"))\n",
    "            rate = (m_size.size()[0] / m_sum).to(torch.device(\"cuda:0\"))\n",
    "            for b in range(out.size()[0]):\n",
    "                out[b] = torch.add(torch.mul(rate[b], torch.sum(torch.mul(masked_x[b], self.W), 1)), self.b)\n",
    "        except IndexError:\n",
    "            m_size = torch.Tensor(mask.size()).to(torch.device(\"cuda:0\"))\n",
    "            m_sum = torch.sum(mask).to(torch.device(\"cuda:0\"))\n",
    "            rate = (m_size.size()[0] / m_sum).to(torch.device(\"cuda:0\"))\n",
    "            out = torch.add(torch.mul(rate, torch.sum(torch.mul(masked_x, self.W))), self.b)\n",
    "        \n",
    "        return self.activation(out)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        #CNN\n",
    "        self.pool = nn.AvgPool2d(2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(1,16,3)\n",
    "        self.conv2 = nn.Conv2d(16,32,3)\n",
    "        self.bn2d1 = nn.BatchNorm2d(16)\n",
    "        self.bn2d2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.fcm = Ignore(32 * 7 * 7 + 43, 32 * 7 * 7 + 43)\n",
    "        self.fcm_c = Ignore(32 * 7 * 7 + 64, 32 * 7 * 7 + 64)\n",
    "        self.bnm = nn.BatchNorm1d(32 * 7 * 7 + 43)\n",
    "        self.bnm_c = nn.BatchNorm1d(32 * 7 * 7 + 64)\n",
    "        #fully connect for hand Location(x,y,z)\n",
    "        self.fcL1 = nn.Linear(32 * 7 * 7 + 43, 300)\n",
    "        self.fcL1_c = nn.Linear(32 * 7 * 7 + 64, 300)\n",
    "        self.fcL2 = nn.Linear(300, 30)\n",
    "        self.fcL3 = nn.Linear(30, 3)        \n",
    "        self.bnL1 = nn.BatchNorm1d(300)\n",
    "        self.bnL2 = nn.BatchNorm1d(30)\n",
    "        #fully connect for hand Pose Descriptor(8 properties)\n",
    "        self.fcPD1 = nn.Linear(32 * 7 * 7 + 43, 300)\n",
    "        self.fcPD1_c = nn.Linear(32 * 7 * 7 + 64, 300)\n",
    "        self.fcPD2 = nn.Linear(300, 60)\n",
    "        self.fcPD3 = nn.Linear(60, 8)\n",
    "        self.bnPD1 = nn.BatchNorm1d(300)\n",
    "        self.bnPD2 = nn.BatchNorm1d(60)\n",
    "        \n",
    "        #Autoencoder\n",
    "        self.mask = Ignore(63, 63)#self.mask = Ignore(84, 84)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense_enc1 = nn.Linear(63, 32)#self.dense_enc1 = nn.Linear(84, 40)\n",
    "        self.bn1 = nn.BatchNorm1d(32)#(40)\n",
    "        self.dense_enc2 = nn.Linear(32, 16)#self.bn1 = nn.BatchNorm1d(40, 18)\n",
    "        self.bn2 = nn.BatchNorm1d(16)#self.bn2 = nn.BatchNorm1d(18)\n",
    "        self.dense_enc3 = nn.Linear(16,8)#self.dense_enc3 = nn.Linear(18, 8)\n",
    "    \n",
    "        self.dense_dec1 = nn.Linear(8,16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        self.dense_dec2 = nn.Linear(16, 26)#self.dense_dec2 = nn.Linear(16, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(26)#self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense_dec3 = nn.Linear(26, 42)#self.dense_dec3 = nn.Linear(32, 63)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        #x = torch.div(x, 100.)\n",
    "        #x = self.mask(x, m)\n",
    "        x = self.dense_enc1(x)\n",
    "        x = self.bn1(self.relu(x))\n",
    "        x = self.dense_enc2(x)\n",
    "        x = self.bn2(self.relu(x))\n",
    "        x = self.dense_enc3(x)\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.tanh(x)\n",
    "        x = self.dense_dec1(x)\n",
    "        x = self.bn4(self.relu(x))\n",
    "        x = self.dense_dec2(x)\n",
    "        x = self.bn5(self.relu(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = self.dense_dec3(x)\n",
    "        #x = self.mask(x, m)\n",
    "        #x = torch.mul(x, 100.)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x=None, z=None):\n",
    "        if x != None and z == None:\n",
    "            z = self.encoder(x)\n",
    "            x = self.decoder(z)\n",
    "        elif x == None and z != None:\n",
    "            print(\"decoder only\")\n",
    "            x = self.decoder(z)\n",
    "        return x, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#出力値保存\n",
    "def outcsv(n_epoch, eval_input_value, eval_output_value, eval_mid_value, adrs):\n",
    "    for b in range(len(eval_input_value)):\n",
    "        fname = f\"dataset_hand_mm_{adrs[b][0]}_{adrs[b][1]}_{adrs[b][2]:03}_{adrs[b][3]:08}_{adrs[b][4]:04}.csv\"\n",
    "        os.makedirs(PATH + \"\\\\outputs\", exist_ok=True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\inputs\", exist_ok=True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\outputs\", exist_ok=True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\mid\", exist_ok=True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\img\", exist_ok = True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\img\\\\input\", exist_ok = True)\n",
    "        os.makedirs(PATH + \"\\\\outputs\\\\img\\\\output\", exist_ok = True)\n",
    "        print(fname)\n",
    "        with open(PATH + \"\\\\outputs\\\\inputs\\\\\" + fname, mode='w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(eval_input_value[b])\n",
    "        with open(PATH + \"\\\\outputs\\\\outputs\\\\\" + fname, mode='w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(eval_output_value[b])\n",
    "        with open(PATH + \"\\\\outputs\\\\mid\\\\\" + fname, mode='w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(eval_mid_value[b])\n",
    "    print(\"fin save.\")\n",
    "    return\n",
    "\n",
    "def saveloss(train, test):\n",
    "    with open(PATH + \"\\\\outputs\\\\loss_values.csv\", mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20200116, 223630, 45, 9004, 1]\n",
      "[20200116, 223630, 45, 9005, 2]\n",
      "[20200116, 223630, 45, 9006, 3]\n",
      "[20200116, 223630, 45, 9007, 4]\n",
      "[20200116, 223630, 45, 9008, 5]\n",
      "[20200116, 223630, 45, 9009, 6]\n",
      "[20200116, 223630, 45, 9010, 7]\n",
      "fin loading.\n",
      "data_sets (7, 2)\n",
      "[ 4.07740e+02 -1.25199e+02 -1.41000e+02  4.51434e-01  3.76318e+02\n",
      " -1.15808e+02 -1.42000e+02  5.91748e-01  3.42703e+02 -1.11767e+02\n",
      " -1.38000e+02  6.71248e-01  3.16491e+02 -1.05619e+02 -1.31915e+02\n",
      "  6.85709e-01  2.90789e+02 -1.05588e+02 -1.25947e+02  4.74967e-01\n",
      "  3.36738e+02 -1.38540e+02 -1.17000e+02  6.30000e-01  3.15059e+02\n",
      " -1.44153e+02 -7.30000e+01  7.03599e-01  3.01896e+02 -1.35344e+02\n",
      " -1.42051e+02  7.89231e-01  2.89918e+02 -1.29200e+02 -2.04888e+02\n",
      "  7.60097e-01  3.47868e+02 -1.39121e+02 -1.31000e+02  5.26194e-01\n",
      "  3.26323e+02 -1.36934e+02 -9.10000e+01  4.20939e-01  3.24031e+02\n",
      " -1.36702e+02 -8.67430e+01  4.09553e-01  3.22336e+02 -1.36530e+02\n",
      " -8.35963e+01  3.17688e-01  3.59205e+02 -1.30427e+02 -1.42000e+02\n",
      "  4.90619e-01  3.41849e+02 -1.20254e+02 -1.48000e+02  3.08848e-01\n",
      "  3.28721e+02 -1.05524e+02 -1.56687e+02  2.50052e-01  3.18339e+02\n",
      " -9.37914e+01 -1.63607e+02  1.73026e-01  3.71110e+02 -1.25133e+02\n",
      " -1.45000e+02  2.95274e-01  3.50082e+02 -1.11815e+02 -1.37000e+02\n",
      "  2.26748e-01  3.44302e+02 -1.06736e+02 -1.46231e+02  2.21058e-01\n",
      "  3.44344e+02 -1.01861e+02 -1.18000e+02  1.13360e-01]\n",
      "[20200116, 223630, 45, 9004, 1]\n",
      "data          train  (7, 84)\n",
      "address       train  (7, 5)\n",
      "7\n",
      "eval\n",
      "dataset_hand_mm_20200116_223630_045_00009004_0001.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009005_0002.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009006_0003.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009007_0004.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009008_0005.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009009_0006.csv\n",
      "dataset_hand_mm_20200116_223630_045_00009010_0007.csv\n",
      "fin save.\n"
     ]
    }
   ],
   "source": [
    "csvlist = glob.glob(dataset_dir + \"\\\\*.csv\")\n",
    "#print(csvlist)\n",
    "data_sets = []\n",
    "inputs = []\n",
    "valid_data_num = 0\n",
    "invalid_data_num = 0\n",
    "valid_label_num = 0\n",
    "invalid_label_num = 0\n",
    "for i in range(len(csvlist)):\n",
    "    points = []\n",
    "    date = []\n",
    "    file_name = csvlist[i]\n",
    "    #dataname = file_name[-51:]\n",
    "    file_name = file_name.split(dataset_dir + \"\\\\dataset_hand_mm_\")\n",
    "    file_name = file_name[1].rsplit(\".csv\")\n",
    "    file_name = file_name[0].split(\"_\")\n",
    "    date.append(int(file_name[0]))\n",
    "    date.append(int(file_name[1]))\n",
    "    date.append(int(file_name[2]))\n",
    "    date.append(int(file_name[4]))\n",
    "    date.append(int(file_name[5]))\n",
    "    #date.append(int(file_name[-35:-27]))\n",
    "    #date.append(int(file_name[-26:-20]))\n",
    "    #date.append(int(file_name[-19:-18]))\n",
    "    #date.append(int(file_name[-17:-9]))\n",
    "    #date.append(int(file_name[-8:-4]))\n",
    "    print(date)\n",
    "    with open(csvlist[i]) as f:\n",
    "        reader = csv.reader(f)\n",
    "        num = 0\n",
    "        for row in reader:\n",
    "            if num == 0:\n",
    "                for point in row:\n",
    "                    if float(point) < -5000.:\n",
    "                        points.append(float(0))\n",
    "                    else:\n",
    "                        points.append(float(point))\n",
    "        points = np.asarray(points)\n",
    "        num += 1\n",
    "    data_pair = []\n",
    "    data_pair.append(points)\n",
    "    data_pair.append(date)\n",
    "    data_sets.append(data_pair)\n",
    "\n",
    "print(\"fin loading.\")\n",
    "data_sets = np.asarray(data_sets)\n",
    "#print(\"data\" + str(data_sets[0]))\n",
    "#print(\"data\", data_label_sets[0][0])\n",
    "#print(\"label\" + str(label_sets[0]))\n",
    "#print(\"label\", data_label_sets[1][0])\n",
    "#print(type(data_sets[0][0]))\n",
    "#print(type(label_sets[0][0]))\n",
    "print(\"data_sets\", data_sets.shape)\n",
    "data_eval = []\n",
    "adrs_eval = []\n",
    "\n",
    "for i in range(len(data_sets)):\n",
    "    data_eval.append(data_sets[i][0])\n",
    "    adrs_eval.append(data_sets[i][1])\n",
    "print(data_eval[0])\n",
    "print(adrs_eval[0])\n",
    "data_evalF = np.array(data_eval).astype('float32')/100\n",
    "adrs_evalF = np.array(adrs_eval)\n",
    "\n",
    "#adrs_evalF = adrs_evalF[:, np.newaxis]\n",
    "\n",
    "print(\"data          train \",data_evalF.shape)\n",
    "print(\"address       train \",adrs_evalF.shape)\n",
    "\n",
    "BATCH_SIZE = data_evalF.shape[0]\n",
    "print(BATCH_SIZE)\n",
    "evalset = Mydatasets(data = data_evalF, adrs = adrs_evalF,)\n",
    "evalloader = torch.utils.data.DataLoader(evalset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "net = Autoencoder()\n",
    "net.load_state_dict(torch.load(PATH + \"\\\\hand_AE_model\"))\n",
    "net = net.to(device)\n",
    "for p in net.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "eval_mid_value = [] #trainingの中間層の出力を保持するlist\n",
    "eval_input_value = []   #trainingの入力を保持するlist\n",
    "eval_output_value = []  #trainingの出力を保持するlist\n",
    "eval_adrs = []\n",
    "\n",
    "eval_size = data_evalF.shape[0]\n",
    "\n",
    "#ここから推定開始\n",
    "print(\"eval\")\n",
    "net.eval()\n",
    "teval = 0\n",
    "for (inputs, adrs) in evalloader:\n",
    "    teval += 1\n",
    "    #物体位置基準座標→手首位置基準座標\n",
    "    handlocs = np.zeros_like(inputs)\n",
    "    #print(inputs.size()[0])\n",
    "    for bsize in range(inputs.size()[0]):\n",
    "        for i in range(int(inputs.size()[1] / 4)):\n",
    "            handlocs[bsize][i*4+0] = (inputs*100)[bsize][0].item()\n",
    "            handlocs[bsize][i*4+1] = (inputs*100)[bsize][1].item()\n",
    "            handlocs[bsize][i*4+2] = (inputs*100)[bsize][2].item()\n",
    "    handlocs = torch.from_numpy(handlocs.astype(np.float32)).clone()\n",
    "    #print(\"handlocs:\", handlocs[0])\n",
    "    #print(\"handlocs:\",handlocs.shape)\n",
    "    inputs_h = ((inputs*100 - handlocs)/100).to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    #print(\"inputs_h:\",inputs_h.shape)\n",
    "    #ネットワークから出力を得る\n",
    "    outputs, mid = net(x = inputs_h)\n",
    "    \n",
    "    #手首位置基準座標→物体位置基準座標\n",
    "    handlocs3d = torch.zeros_like(outputs).to(device)\n",
    "    #print(outputs.size()[0])\n",
    "    for bsize in range(outputs.size()[0]):\n",
    "        for i in range(int(handlocs.size()[1] / 4)):\n",
    "            handlocs3d[bsize][i*3+0] = handlocs[bsize][i*4+0]\n",
    "            handlocs3d[bsize][i*3+1] = handlocs[bsize][i*4+1]\n",
    "            handlocs3d[bsize][i*3+2] = handlocs[bsize][i*4+2]\n",
    "    outputs_o = (outputs*100 + handlocs3d).to(device)\n",
    "    #print(\"handlocs3d:\",handlocs3d.shape)\n",
    "    #print(\"inputs:\",inputs.shape)\n",
    "    #print(\"outputs_o:\",outputs_o.shape)\n",
    "    #print(\"mid:\",mid.shape)\n",
    "    \n",
    "    eval_input_value = (inputs*100).to(device_cpu).detach().numpy().copy()\n",
    "    eval_output_value = outputs_o.to(device_cpu).detach().numpy().copy()\n",
    "    eval_mid_value = mid.to(device_cpu).detach().numpy().copy()\n",
    "#print(len(eval_input_value))\n",
    "#print(len(eval_output_value))\n",
    "#print(len(eval_mid_value))\n",
    "#print(eval_input_value[0])\n",
    "#print(eval_output_value[0])\n",
    "#print(eval_mid_value[0])\n",
    "outcsv(1, eval_input_value, eval_output_value, eval_mid_value, adrs)\n",
    "#eval_input_value.clear()\n",
    "#eval_output_value.clear()\n",
    "#eval_mid_value.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
