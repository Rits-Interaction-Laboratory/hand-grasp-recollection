{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    #グラフ出力用module\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "WEIGHT_DECAY = 0.005\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 100\n",
    "PATH = \"C:\\\\Users\\\\arimoto\\\\ObjectOperate\"\n",
    "#TRAINPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\train\"\n",
    "#TESTPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\test\"\n",
    "#VALIDPATH = \"C:\\\\Users\\\\arimoto\\\\cnntest\\\\valid\"\n",
    "\n",
    "image_size=36\n",
    "#object_name = \"object4\"\n",
    "\n",
    "datadirs = glob.glob(PATH + \"\\\\dataset\\\\*\")\n",
    "imgfiles = glob.glob(PATH + \"\\\\\" + object_name + \"\\\\\"+ object_name + \".png\")\n",
    "csvdatafiles = glob.glob(PATH + \"\\\\\" + object_name + \"\\\\data\\\\*.csv\")\n",
    "csvlabelfiles = glob.glob(PATH + \"\\\\\" + object_name + \"\\\\label\\\\*.csv\")\n",
    "\n",
    "img_arrays = []\n",
    "img_names = []\n",
    "datas = []\n",
    "labels = []\n",
    "\n",
    "for imgfile in imgfiles:\n",
    "    # 画像読み込み\n",
    "    image = Image.open(imgfile)\n",
    "    # グレイスケール変換\n",
    "    image = image.convert('L')\n",
    "    # リサイズ\n",
    "    #image = image.resize((image_size, image_size))\n",
    "    # 画像から配列に変換\n",
    "    img_data = np.asarray(image)\n",
    "    img_names.append(os.path.basename(imgfile))\n",
    "    #file_split = [i for i in file.split('_')]\n",
    "    img_arrays.append(img_data)\n",
    "    \n",
    "for datafile in csvdatafiles:\n",
    "    with open(datafile) as f:\n",
    "        reader = csv.reader(f)\n",
    "        num = 0\n",
    "        for row in reader:\n",
    "            if num == 0:\n",
    "                points = []\n",
    "                for point in row:\n",
    "                    points.append(int(point))\n",
    "                datas.append(np.asarray(points))\n",
    "                num += 1\n",
    "for labelfile in csvlabelfiles:\n",
    "    with open(labelfile) as f:\n",
    "        reader = csv.reader(f)\n",
    "        num = 0\n",
    "        for row in reader:\n",
    "            if num == 0:\n",
    "                #print(row)\n",
    "                points = []\n",
    "                for point in row:\n",
    "                    points.append(int(point))\n",
    "                labels.append(np.asarray(points))\n",
    "                num += 1\n",
    "#labels.append((int(file_split[2]), int(file_split[3])))\n",
    "    \n",
    "print(\"fin loading\")\n",
    "datas = np.array(datas)\n",
    "labels = np.array(labels)\n",
    "img_arrays = np.array(img_arrays)\n",
    "print(\"data\" + str(datas))\n",
    "print(\"label\" + str(labels))\n",
    "print(\"image_names\" + str(img_names))\n",
    "#print(\"image_arrays\" + str(img_arrays))\n",
    "#for i in range(img_arrays.shape[1]):\n",
    "#    print(img_arrays.shape)\n",
    "#    print(img_arrays[0][i])\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# trainデータとtestデータに分割\n",
    "data_train, data_test, label_train, label_test = train_test_split(\n",
    "    datas,\n",
    "    labels,\n",
    "    random_state = 0,\n",
    "    test_size = 0.2\n",
    ")\n",
    "print(data_train.shape, label_train.shape, data_test.shape, label_test.shape)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# データ型の変換＆正規化\n",
    "data_train = data_train.astype('float32')# / 255\n",
    "data_test = data_test.astype('float32')# / 255\n",
    "img_array = img_arrays[0].astype('float32')/255\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "trans = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()]#,torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
    ")\n",
    "\n",
    "print(data_train[0])\n",
    "print(label_train[0])\n",
    "print(data_test[0])\n",
    "print(label_test[0])\n",
    "print(img_array)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, datas, labels, img_array, transform = None):\n",
    "        self.transform = transform\n",
    "        #self.transform2 = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "        self.data = datas\n",
    "        self.label = labels\n",
    "        self.img_array = img_array\n",
    "\n",
    "        self.datanum = datas.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i_data = self.data[idx]\n",
    "        i_label = self.label[idx]\n",
    "        \n",
    "        #print(out_label)\n",
    "        #print(type(i_label))\n",
    "        out_label = np.array(i_label.astype(np.float32))\n",
    "        out_data = np.array(i_data.astype(np.float32))\n",
    "        #out_label.append(i_label.astype(np.float32))\n",
    "        #print(type(out_label))\n",
    "\n",
    "        if self.transform:\n",
    "            #out_data = self.transform(i_data)\n",
    "            out_img = self.transform(self.img_array)\n",
    "            #out_label = self.transform2(out_label)\n",
    "\n",
    "        return out_data, out_label, out_img\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "trainset = Mydatasets(datas = data_train, labels = label_train, img_array = img_array, transform = trans)\n",
    "testset = Mydatasets(datas = data_test, labels = label_test, img_array = img_array, transform = trans)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE,\n",
    "                        shuffle = True, num_workers = 0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = BATCH_SIZE,\n",
    "                        shuffle = False, num_workers = 0)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,16,3)\n",
    "        self.conv2 = nn.Conv2d(16,32,3)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7 + 11, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        #print(\"x_shape: \", x.size())\n",
    "        #print(\"y_shape: \", y.size())\n",
    "        x = torch.cat([x, y], axis = -1)\n",
    "        #print(\"torch.cat(x, y): \", x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_loss_value=[]      #trainingのlossを保持するlist\n",
    "#train_acc_value=[]       #trainingのaccuracyを保持するlist\n",
    "test_loss_value=[]       #testのlossを保持するlist\n",
    "#test_acc_value=[]        #testのaccuracyを保持するlist \n",
    "\n",
    "max_train_loss_value = 0.\n",
    "max_test_loss_value = 0.\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch', epoch+1)    #epoch数の出力\n",
    "    for (inputs, labels, input_img) in trainloader:\n",
    "        inputs, labels, input_img = inputs.to(device), labels.to(device), input_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"inputs:\", inputs)\n",
    "        outputs = net(input_img, inputs)\n",
    "        #if epoch%10 == 0:\n",
    "            #print(\"outputs:\", outputs)\n",
    "            #print(\"labels:\", labels)\n",
    "        \n",
    "#        out_sums = []\n",
    "#        lab_sums = []\n",
    "#        for out in outputs:\n",
    "#            out_sums.append(out[0].item()+out[1].item())\n",
    "#        for lab in labels:\n",
    "#            lab_sums.append(lab[0].item()+lab[1].item())\n",
    "#        print(out_sums)\n",
    "#        print(lab_sums)\n",
    "#        crit_outputs = torch.tensor(np.array(out_sums))\n",
    "#        crit_labels = torch.tensor(np.array(lab_sums))\n",
    "        \n",
    "        loss = criterion(outputs, labels)#crit_outputs, crit_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    sum_loss = 0.0          #lossの合計\n",
    "    #sum_correct = 0         #正解率の合計\n",
    "    sum_total = 0           #dataの数の合計\n",
    "\n",
    "    #train dataを使ってテストをする(パラメータ更新がないようになっている)\n",
    "    for (inputs, labels, input_img) in trainloader:\n",
    "        inputs, labels, input_img = inputs.to(device), labels.to(device), input_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(input_img, inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        sum_loss += loss.item()                            #lossを足していく\n",
    "        #_, predicted = outputs.max(1)                      #出力の最大値の添字(予想位置)を取得\n",
    "        sum_total += labels.size(0)                        #labelの数を足していくことでデータの総和を取る\n",
    "        #sum_correct += (predicted == labels).sum().item()  #予想位置と実際の正解を比べ,正解している数だけ足す\n",
    "    print(\"train mean loss={}\"\n",
    "            .format(sum_loss*BATCH_SIZE/len(trainloader.dataset)))  #loss出力\n",
    "    train_loss_value.append(sum_loss*BATCH_SIZE/len(trainloader.dataset))  #traindataのlossをグラフ描画のためにlistに保持\n",
    "    #train_acc_value.append(float(sum_correct/sum_total))   #traindataのaccuracyをグラフ描画のためにlistに保持\n",
    "    if sum_loss*BATCH_SIZE/len(trainloader.dataset) > max_train_loss_value:\n",
    "        max_train_loss_value = sum_loss*BATCH_SIZE/len(trainloader.dataset)\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    sum_correct = 0\n",
    "    sum_total = 0\n",
    "\n",
    "    #test dataを使ってテストをする\n",
    "    for (inputs, labels, input_img) in testloader:\n",
    "        inputs, labels, input_img = inputs.to(device), labels.to(device), input_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(input_img, inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        sum_loss += loss.item()\n",
    "        #_, predicted = outputs.max(1)\n",
    "        sum_total += labels.size(0)\n",
    "        #sum_correct += (predicted == labels).sum().item()\n",
    "    print(\"test mean loss={}\"\n",
    "            .format(sum_loss*BATCH_SIZE/len(testloader.dataset)))\n",
    "    test_loss_value.append(sum_loss*BATCH_SIZE/len(testloader.dataset))\n",
    "    #test_acc_value.append(float(sum_correct/sum_total))\n",
    "    if sum_loss*BATCH_SIZE/len(testloader.dataset) > max_test_loss_value:\n",
    "        max_test_loss_value = sum_loss*BATCH_SIZE/len(testloader.dataset)\n",
    "\n",
    "plt.figure(figsize=(6,6))      #グラフ描画用\n",
    "ylim = max(max_train_loss_value, max_test_loss_value)\n",
    "\n",
    "#以下グラフ描画\n",
    "plt.plot(range(EPOCH), train_loss_value)\n",
    "plt.plot(range(EPOCH), test_loss_value, c='#00ff00')\n",
    "plt.xlim(0, EPOCH)\n",
    "plt.ylim(0, ylim)\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['train loss', 'test loss'])\n",
    "plt.title('loss')\n",
    "plt.savefig(PATH + \"\\\\\" + object_name + \"\\\\loss_image.png\")\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "a = torch.ones((16, 5))\n",
    "b = torch.zeros((16, 3))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "torch.cat([a, b], axis=-1).shape\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#input_img_test = trans(img_array)\n",
    "#print(input_img_test)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "label_points = []\n",
    "label_view = []\n",
    "\n",
    "for l in range(10):\n",
    "    label_points.append(0)\n",
    "label_view.append(label_points)\n",
    "label_view = np.array(label_view)\n",
    "\n",
    "def view_sequence(points):\n",
    "    data_view = []\n",
    "    data_view.append(points)\n",
    "    data_view = np.array(data_view)\n",
    "    #print(data_view.shape, label_view.shape)\n",
    "    data_view = data_view.astype('float32')# / 255\n",
    "    #init_l_points = np.array(init_l_points.astype(np.float32))\n",
    "    #print(init_l_points)\n",
    "    viewset = Mydatasets(datas = data_view, labels = label_view, img_array = img_array, transform = trans)\n",
    "    viewloader = torch.utils.data.DataLoader(viewset, batch_size = 1, shuffle = False, num_workers = 0)\n",
    "    \n",
    "    for (inputs, labels, input_img) in viewloader:\n",
    "        inputs, labels, input_img = inputs.to(device), labels.to(device), input_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        view_out = net(input_img, inputs)\n",
    "        #print(view_out)\n",
    "    view_out_np = view_out.to('cpu').detach().numpy().copy()\n",
    "    #print(view_out_np[0])\n",
    "    return view_out_np[0]\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "init_l_points = []\n",
    "\n",
    "#5つのランダムな点を作成(x, y)\n",
    "fin1x = random.randint(-80, -11)\n",
    "fin2x = random.randint(-80, -11)\n",
    "fin3x = random.randint(-80, -11)\n",
    "fin4x = random.randint(-80, -11)\n",
    "fin5x = random.randint(-80, -11)\n",
    "fin1y = random.randint(-80, 80)\n",
    "fin2y = random.randint(-80, 80)\n",
    "fin3y = random.randint(-80, 80)\n",
    "fin4y = random.randint(-80, 80)\n",
    "fin5y = random.randint(-80, 80)\n",
    "init_l_points.append(fin1x)\n",
    "init_l_points.append(fin1y)\n",
    "init_l_points.append(fin2x)\n",
    "init_l_points.append(fin2y)\n",
    "init_l_points.append(fin3x)\n",
    "init_l_points.append(fin3y)\n",
    "init_l_points.append(fin4x)\n",
    "init_l_points.append(fin4y)\n",
    "init_l_points.append(fin5x)\n",
    "init_l_points.append(fin5y)\n",
    "init_l_points.append(0)\n",
    "l_points = np.asarray(init_l_points)\n",
    "print(l_points)\n",
    "\n",
    "with open(PATH + \"\\\\\" + object_name + \"\\\\view_l_0.csv\" , 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(l_points.tolist())\n",
    "    \n",
    "for v in range(10):\n",
    "    l_points = view_sequence_flip(l_points)\n",
    "    #print(l_points)\n",
    "    l_points = l_points.tolist()\n",
    "    l_points.append(v+1)\n",
    "    print(l_points)\n",
    "    with open(PATH + \"\\\\\" + object_name + \"\\\\view_l_\" + str(v+1) + \".csv\" , 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(l_points)\n",
    "    l_points = np.array(l_points)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "init_r_points = []\n",
    "\n",
    "#5つのランダムな点を作成(x, y)\n",
    "fin1x = random.randint(20, 80)\n",
    "fin2x = random.randint(20, 80)\n",
    "fin3x = random.randint(20, 80)\n",
    "fin4x = random.randint(20, 80)\n",
    "fin5x = random.randint(20, 80)\n",
    "fin1y = random.randint(-80, 80)\n",
    "fin2y = random.randint(-80, 80)\n",
    "fin3y = random.randint(-80, 80)\n",
    "fin4y = random.randint(-80, 80)\n",
    "fin5y = random.randint(-80, 80)\n",
    "init_r_points.append(fin1x)\n",
    "init_r_points.append(fin1y)\n",
    "init_r_points.append(fin2x)\n",
    "init_r_points.append(fin2y)\n",
    "init_r_points.append(fin3x)\n",
    "init_r_points.append(fin3y)\n",
    "init_r_points.append(fin4x)\n",
    "init_r_points.append(fin4y)\n",
    "init_r_points.append(fin5x)\n",
    "init_r_points.append(fin5y)\n",
    "init_r_points.append(0)\n",
    "r_points = np.asarray(init_r_points)\n",
    "print(r_points)\n",
    "\n",
    "with open(PATH + \"\\\\\" + object_name + \"\\\\view_r_0.csv\" , 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(r_points.tolist())\n",
    "\n",
    "for v in range(10):\n",
    "    r_points = view_sequence_flip(r_points)\n",
    "    #print(l_points)\n",
    "    r_points = r_points.tolist()\n",
    "    r_points.append(v+1)\n",
    "    print(r_points)\n",
    "    with open(PATH + \"\\\\\" + object_name + \"\\\\view_r_\" + str(v+1) + \".csv\" , 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(r_points)\n",
    "    r_points = np.array(r_points)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from PIL import ImageOps\n",
    "\n",
    "img_arrays_flip = []\n",
    "imgfile_flip = PATH + \"\\\\\" + object_name + \"\\\\\"+ object_name + \".png\"\n",
    "\n",
    "# 画像読み込み\n",
    "image_flip = Image.open(imgfile_flip)\n",
    "#左右反転処理\n",
    "image_flip = ImageOps.mirror(image_flip)\n",
    "# グレイスケール変換\n",
    "image_flip = image_flip.convert('L')\n",
    "# リサイズ\n",
    "#image = image.resize((image_size, image_size))\n",
    "# 画像から配列に変換\n",
    "img_data_flip = np.asarray(image_flip)\n",
    "#img_names.append(os.path.basename(imgfile))\n",
    "#file_split = [i for i in file.split('_')]\n",
    "img_arrays_flip.append(img_data_flip)\n",
    "img_arrays_flip = np.array(img_arrays_flip)\n",
    "img_array_flip = img_arrays_flip[0].astype('float32')/255\n",
    "\n",
    "def view_sequence_flip(points):\n",
    "    data_view = []\n",
    "    data_view.append(points)\n",
    "    data_view = np.array(data_view)\n",
    "    #print(data_view.shape, label_view.shape)\n",
    "    data_view = data_view.astype('float32')# / 255\n",
    "    #init_l_points = np.array(init_l_points.astype(np.float32))\n",
    "    #print(init_l_points)\n",
    "    viewset = Mydatasets(datas = data_view, labels = label_view, img_array = img_array_flip, transform = trans)\n",
    "    viewloader = torch.utils.data.DataLoader(viewset, batch_size = 1, shuffle = False, num_workers = 0)\n",
    "    \n",
    "    for (inputs, labels, input_img) in viewloader:\n",
    "        inputs, labels, input_img = inputs.to(device), labels.to(device), input_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        view_out = net(input_img, inputs)\n",
    "        #print(view_out)\n",
    "    view_out_np = view_out.to('cpu').detach().numpy().copy()\n",
    "    #print(view_out_np[0])\n",
    "    return view_out_np[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
